\section{Fundamentos de las redes neuronales convolucionales}
Desde el punto de vista del aprendizaje automático, se ha abordado con múltiples herramientas y modelos el problema de la clasificación automática de imágenes. Los avances sucedidos en el campo, también han tenido un efecto en dentro de la clasificación automática de imágenes. Dentro de esos avances, hay que destacar la aparición de la técnica ``Back Propagation'' que hizo posible el entrenamiento de redes neuronales de gran profundidad. Este tipo de modelos, que tratan de imitar el comportamiento de un cerebro, han producido un gran avance en este campo.\\

Las redes neuronales convolucionales, dan un paso más allá, obteniendo grandes resultados en los tipos de datos en el que la información de un punto está especialmente vinculada a la de sus vecinos, como pasa en las imágenes.\\
 
Dada la gran utilidad e importancia de este modelo concreto para la tarea, se definirán sus componentes, y se realizará una pequeña comparación con las redes neuronales usuales para tratar de mostrar sus ventajas.\\

\subsection{Definición de neurona artificial}

Por ser un modelo bioinspirado, primero es necesario conocer el funcionamiento básico de una neurona natural. Las neuronas son la unidad básica de nuestro cerebro. En él, podemos encontrar aproximadamente 86 mil millones de neuronas, conectadas en entre ellas mediante $10^{14}$ y $10^{15}$ sinapsis. En la figura \todo[color=red!50]{Figura de una neurona al canto} podemos ver las diferentes partes de una neurona. Las dendritas funcionan como la entrada de información por medio de la sinapsis con otras neuronas. El impulso es transmitido al núcleo de la neurona y tras esto, la señal de salida es emitida por el axión hacia otras neuronas. Estudiaremos el proceso paso a paso definiendo en cada momento el equivalente de la neurona artificial, para llegar a la fórmula final que la representa.\\

La entrada de una neurona artificial será por tanto un conjunto de valores $\left\lbrace x_1,x_2,\ldots,x_N\right\rbrace$ con $x_n \in \mathbb{R}$, para $n = 1,\ldots,N$. Las dendritas de la neurona, pueden ser excitadas, facilitando la activación de la neurona o inhibidas, dificultándolo. Para simular esto, se añaden una serie de pesos $\left\lbrace w_1,w_2,\ldots,w_N\right\rbrace$ con $w_n \in \mathbb{R}$, para $n = 1,\ldots,N$, que multiplicarán a las entradas y un valor adicional $b \in \mathbb{R}$, llamado bías, que representa la facilidad de la neurona para activarse. Los pesos y el bías son de una gran importancia para el aprendizaje de la red, pues una vez definida la red, son los únicos elementos que se puede aprender. Esto simula también el aprendizaje humano, que se realiza mediante el cambio de comportamiento de las neuronas y de como se afectan unas a otras, haciendo que puedan pasar a inhibir, excitar o incluso a desaparecer el enlace por carecer de importancia, simulado en las redes neuronales como un peso 0.\\

Tras esto núcleo de la neurona combina las entradas que recibe. En el modelo de la neurona artificial, esta combinación se realiza mediante la suma de las entradas, es decir: $$\sum^N_{n=1} x_n w_n + b. $$

La activación de la neurona es un proceso brusco. Si pasa de un umbral, se activa. Este proceso se modela en la práctica con una gran cantidad de funciones. La discusión sobre que función elegir tiene más que ver con el entrenamiento de las redes en su conjunto que con el funcionamiento de la neurona.\\

Por ahora, se toma la función clásica en este campo, la sigmoide, $\sigma(x)$ definida de la siguiente manera:

$$\sigma: \mathbb{R}\longrightarrow (0,1)$$
$$\sigma(x)= \frac{1}{1+e^{-t}}$$

Juntando todos los elementos, representamos una neurona artificial mediante la siguiente ecuación:

$$\sigma \left( \sum^N_{n=1} x_n w_n + b. \right).$$

\subsection{Redes Neuronales Artificiales}

Siguiendo con el modelo inspirado en el cerebro, el siguiente paso son las redes neuronales. Tras ver el modelo de una neurona, y el comportamiento de una neurona real, podemos apreciar que la capacidad de aprendizaje de este elemento es relativamente pequeño. La verdadera capacidad reside en la unión de las neuronas en conjuntos interconectados de las mismas, llamadas redes neuronales.\\
 
Las redes neuronales artificiales simplifican esta realidad, incluyendo las siguientes restricciones:
\begin{itemize}
\item \textbf{Organización por capas:} Las redes neuronales artificiales se organizan por capas. Es decir, la entrada de la capa i, son las salidas de la capa i-1, y las salidas de la capa i, serán las entradas de la capa i+1. Esta organización tan forzada y artificial no se cumple así en los cerebros reales. 
\item \textbf{No hay retroalimentación:} Las neuronas no pueden conectarse a capas anteriores. Esta restricción se incluye se podría encajar dentro de la anterior, pero tiene importancia propia. La retroalimentación sí sucede en la realidad, y se cree que es importante para el funcionamiento de un cerebro. Existen modelos que incluyen este fenómeno, pero no son tan populares como este modelo simplificado. 
\end{itemize}

El modelo de las redes neuronales artificiales es bastante claro tras estas restricciones con respecto a las redes de neuronas naturales. Se definen de 3 tipos de capas distintas:
\begin{itemize}
\item \textbf{Capa de entrada:} 
\item \textbf{Capa oculta:}
\item \textbf{Capa de salida:}
\end{itemize}

Una red neuronal consta de una primera capa de entrada, seguida por varias capas ocultas, terminando en una capa de salida. En la figura \todo[color=red!50]{Introducir imagen de red neuronal} podemos ver un ejemplo de red neuronal artificial.\\

\subsubsection{Propiedades}

Estas redes crecieron en popularidad por su increíble potencia a la hora de poder aproximar cualquier función real. \todo[color=red!50]{Introducir artículos de potencia de RNA}\\

Sin embargo, las redes de una capas oculta no han conseguido ser un éxito a nivel práctico. Estaban limitados por dos factores muy importantes. \\

Por un lado, el resultado teórico no nos informa del número de neuronas que tenemos en la capa intermedia y aún suponiendo que pudiéramos conseguirlo, en el marco del aprendizaje automático no conocemos la función que queremos aproximar. Por tanto, pese a conocer esta gran propiedad, no la podemos utilizar al no saber si el tamaño de la capa será suficiente.\\

Por el otro, ya vimos que los modelos que tienen una gran capacidad para aprender tienden a sobreaprender. Una segunda mirada a la propiedad podría ser que tiene una capacidad infinita de aprender. Esto lleva a pensar que si la capa intermedia tiene demasiadas neuronas, sobreaprenderá, lo cual es un factor negativo. Este factor quizás fue el más importante para que no fuera un éxito total a nivel práctico.\\

La solución parecía estar entonces en un aumento de capas ocultas. El razonamiento era que se esperaba que, a más capas se introdujeran en el modelo, las sucesivas capas obtendrían como datos de entrada funciones más complejas y útiles, pues si no fueran más útiles no las aprenderían la red, y sin necesidad de aumentar tanto el número de neuronas. Pero este proceso era imposible de llevar a cabo, pues la implementación del gradiente para redes neuronales profundas era tremendamente ineficiente a nivel computacional, haciendo inviable su uso.\\

\subsection{Back Propagation y Deep Learning}


\subsection{Redes neuronales convolucionales}


\subsection{Comparativa de redes neuronales}
