\section{Fundamentos de las redes neuronales convolucionales}
Desde el punto de vista del aprendizaje automático, se ha abordado con múltiples herramientas y modelos el problema de la clasificación automática de imágenes. Los avances sucedidos en el campo, también han tenido un efecto en dentro de la clasificación automática de imágenes. Dentro de esos avances, hay que destacar la aparición de la técnica ``Back Propagation'' que hizo posible el entrenamiento de redes neuronales de gran profundidad. Este tipo de modelos, que tratan de imitar el comportamiento de un cerebro, han producido un gran avance en este campo.\\

Las redes neuronales convolucionales, dan un paso más allá, obteniendo grandes resultados en los tipos de datos en el que la información de un punto está especialmente vinculada a la de sus vecinos, como pasa en las imágenes.\\
 
Dada la gran utilidad e importancia de este modelo concreto para la tarea, se definirán sus componentes, y se realizará una pequeña comparación con las redes neuronales usuales para tratar de mostrar sus ventajas.\\

\subsection{Definición de neurona artificial}

Por ser un modelo bioinspirado, primero es necesario conocer el funcionamiento básico de una neurona natural. Las neuronas son la unidad básica de nuestro cerebro. En él, podemos encontrar aproximadamente 86 mil millones de neuronas, conectadas en entre ellas mediante $10^{14}$ y $10^{15}$ sinapsis. En la figura \todo[color=red!50]{Figura de una neurona al canto} podemos ver las diferentes partes de una neurona. Las dendritas funcionan como la entrada de información por medio de la sinapsis con otras neuronas. El impulso es transmitido al núcleo de la neurona y tras esto, la señal de salida es emitida por el axión hacia otras neuronas. Estudiaremos el proceso paso a paso definiendo en cada momento el equivalente de la neurona artificial, para llegar a la fórmula final que la representa.\\

La entrada de una neurona artificial será por tanto un conjunto de valores $\left\lbrace x_1,x_2,\ldots,x_N\right\rbrace$ con $x_n \in \mathbb{R}$, para $n = 1,\ldots,N$. Las dendritas de la neurona, pueden ser excitadas, facilitando la activación de la neurona o inhibidas, dificultándolo. Para simular esto, se añaden una serie de pesos $\left\lbrace w_1,w_2,\ldots,w_N\right\rbrace$ con $w_n \in \mathbb{R}$, para $n = 1,\ldots,N$, que multiplicarán a las entradas y un valor adicional $b \in \mathbb{R}$, llamado bías, que representa la facilidad de la neurona para activarse. Los pesos y el bías son de una gran importancia para el aprendizaje de la red, pues una vez definida la red, son los únicos elementos que se puede aprender. Esto simula también el aprendizaje humano, que se realiza mediante el cambio de comportamiento de las neuronas y de como se afectan unas a otras, haciendo que puedan pasar a inhibir, excitar o incluso a desaparecer el enlace por carecer de importancia, simulado en las redes neuronales como un peso 0.\\

Tras esto núcleo de la neurona combina las entradas que recibe. En el modelo de la neurona artificial, esta combinación se realiza mediante la suma de las entradas, es decir: $$\sum^N_{n=1} x_n w_n + b. $$

La activación de la neurona es un proceso brusco. Si pasa de un umbral, se activa. Este proceso se modela en la práctica con una gran cantidad de funciones. La discusión sobre que función elegir tiene más que ver con el entrenamiento de las redes en su conjunto que con el funcionamiento de la neurona. La única restricción que se le impone es que sea diferenciable.\\

Juntando todos los elementos, representamos una neurona artificial mediante la siguiente ecuación:

$$f\left( \sum^N_{n=1} x_n w_n + b. \right).$$

\subsection{Redes Neuronales Artificiales}

Siguiendo con el modelo inspirado en el cerebro, el siguiente paso son las redes neuronales. Tras ver el modelo de una neurona, y el comportamiento de una neurona real, podemos apreciar que la capacidad de aprendizaje de este elemento es relativamente pequeño. La verdadera capacidad reside en la unión de las neuronas en conjuntos interconectados de las mismas, llamadas redes neuronales.\\
 
Las redes neuronales artificiales simplifican esta realidad, incluyendo las siguientes restricciones:
\begin{itemize}
\item \textbf{Organización por capas:} Las redes neuronales artificiales se organizan por capas. Es decir, la entrada de la capa i, son las salidas de la capa i-1, y las salidas de la capa i, serán las entradas de la capa i+1. Esta organización tan forzada y artificial no se cumple así en los cerebros reales. 
\item \textbf{No hay retroalimentación:} Las neuronas no pueden conectarse a capas anteriores. Esta restricción se incluye se podría encajar dentro de la anterior, pero tiene importancia propia. La retroalimentación sí sucede en la realidad, y se cree que es importante para el funcionamiento de un cerebro. Existen modelos que incluyen este fenómeno, pero no son tan populares como este modelo simplificado. 
\end{itemize}

El modelo de las redes neuronales artificiales es bastante claro tras estas restricciones con respecto a las redes de neuronas naturales. Se definen de 3 tipos de capas distintas:
\begin{itemize}
\item \textbf{Capa de entrada:} 
\item \textbf{Capa oculta:}
\item \textbf{Capa de salida:}
\end{itemize}

Una red neuronal consta de una primera capa de entrada, seguida por varias capas ocultas, terminando en una capa de salida. En la figura \todo[color=red!50]{Introducir imagen de red neuronal} podemos ver un ejemplo de red neuronal artificial. Se puede denotar la arquitectura de la red por el número de capas ocultas y de salida que tenga. Se hace así al obviar dentro de la arquitectura la capa de entrada, por no tener pesos en comparación a las otras dos.\\

\subsubsection{Propiedades}

Estas redes son muy generales. Las redes neuronales artificiales de dos capas son aproximadores universales. Es decir, una red neuronal de dos capas puede aproximar uniformemente cualquier función continua en un dominio de entrada compacto, con una precisión arbitraria dada, si tiene suficientes neuronas en la capa oculta\cite{Cybenco}\cite{Hornik}.\\

Sin embargo, las redes de una capas oculta no han conseguido ser un éxito a nivel práctico. El resultado teórico no nos informa del número de neuronas que tenemos en la capa intermedia y aún suponiendo que conocida una función, pudiéramos saber el número de neuronas necesaria, en el marco del aprendizaje automático no conocemos la función que queremos aproximar. Por tanto, pese a conocer esta propiedad, no la podemos utilizar al no saber si el tamaño de la capa será suficiente.\\

Por el otro, ya vimos que los modelos que tienen una gran capacidad para aprender pueden también sobreaprender. Una segunda mirada a la propiedad podría ser que tiene una capacidad infinita de aprender. Esto lleva a pensar que si la capa intermedia tiene demasiadas neuronas, sobreaprenderá, lo cual puede ser factor negativo.\\


\subsection{Entrenamiento de una red neuronal}

Se ha hecho referencia varias veces a la importancia que tiene el proceso de aprendizaje en las redes neuronales, y la necesidad de que sea eficiente.

\subsection{Deep Learning}

El proceso habitual en el mundo del aprendizaje automático estaba limitado en su habilidad para procesar datos naturales directamente. Durante mucho tiempo hizo falta un minucioso proceso para diseñar un conjunto de características que transformaran los datos de entrada en una representación aceptable para el sistema de aprendizaje\cite{lecun-nature}.\\

Se llama aprendizaje de la representación a un conjunto de métodos que automáticamente tratan de reconocer estas representaciones optimas para los posteriores métodos de aprendizaje. Deep Learning se encaja dentro de este tipo de métodos. En él, la composición de varias capas de funciones simples pero no lineales, hace que cada capa obtenga una representación más compleja y de más alto nivel. Con la composición de muchas de estas capas, se pueden aprender funciones muy muy complejas. El aspecto fundamental del Deep Learning es que las características obtenidas no han sido diseñadas por un ingeniero, se aprenden automáticamente siguiendo un procedimiento de caracter general.\\
 
Esta técnica ha conseguido grandes avances en el mundo de la inteligencia artificial. Es extremadamente buena encontrando estructuras complejas dentro de los datos de alta dimensionalidad, haciéndola especialmente útil para muchos dominios. En concreto, es una de las partes fundamentales de los grandes resultados que se obtienen en el campo de la clasificación de imágenes. Evidentemente, este problema encaja dentro del caso anterior: datos con una gran dimensionalidad y características complejas que eran necesarias diseñar por los ingenieros.\\

Pero la aplicación directa de estos métodos, pese a ser efectiva, puede tener algunos problemas con respecto a las imágenes. Imaginemos el problema de clasificar imágenes relativamente pequeñas, de 32x32 píxeles RGB. Tendríamos entonces 3072 datos de entrada. Por cada neurona de la primera capa, necesitaríamos 3073 pesos. Es fácil ver lo rápido que puede escalar el número de pesos al aumentar el tamaño de la red, cosa que por otra parte es necesario en un espacio de datos tan complejo.\\

En esa visión, sin embargo, se obvia una importante característica de las imágenes, la importancia de la posición de un dato. Imaginemos que queremos detectar en imágenes de fotografías de carnet los ojos de las personas que aparecen. Podríamos, en vez de intentar analizar en búsqueda de un ojo la imagen en su conjunto, tomar una ventana del tamaño de un ojo, y pasarla por toda la imagen buscándolos. Por supuesto, al ser la ventana más pequeña, la red necesitará menos pesos para poder funcionar correctamente. Esta es la idea detrás de las redes neuronales convolucionales que introdujo el investigador Yann Lecun \cite{lecun-89e}\cite{lecun-98}.\\ 

\subsection{Redes neuronales convolucionales}


\subsection{Comparativa de redes neuronales}
